{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"ImageCore\")\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using Images, ImageCore\n",
    "using LinearAlgebra, Statistics, Plots\n",
    "using NLopt, JuMP\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We consider the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset from [Zalando Research](https://github.com/zalandoresearch).\n",
    "\n",
    "It contains images of Fashion items from 9 categories.\n",
    "\n",
    "In Julia this dataset can be loaded through the [MLDatasets](https://github.com/JuliaML/MLDatasets.jl) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, Y_all = FashionMNIST(split=:train)[:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `X_all` contains the input data and `Y_all` the output data of the training data in the `FashionMNIST` dataset.\n",
    "\n",
    "The entries of `Y_all` correspond to the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \n",
    "            \"Trouser\", \n",
    "            \"Pullover\", \n",
    "            \"Dress\", \n",
    "            \"Coat\", \n",
    "            \"Sandal\", \n",
    "            \"Shirt\", \n",
    "            \"Sneaker\", \n",
    "            \"Bag\", \n",
    "            \"Ankle boot\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `Julia` indexing starts at `1` and `Y_all` contains values between `0` and `9`, it will be convenient to add 1 to all values `Y_all`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all = Y_all .+ 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at 6 randomly chosen images along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = rand(1:60000, 6)\n",
    "println([\"$(labels[Y_all[i]])\" for i in k])\n",
    "[convert2image(FashionMNIST, X_all[:,:,i]) for i in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing two categories\n",
    "\n",
    "To illustrate how the classification methods from the lecture work we consider only two items, namely Coats and Sandals. The goal is to learn an algorithm that can distinguish pictures from these two categories.\n",
    "\n",
    "First, we compute the locations of Coats and Sandals in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l₁ = findfirst(labels .== \"Coat\")\n",
    "l₂ = findfirst(labels .== \"Sandal\")\n",
    "l = findall(y -> y == l₁ || y == l₂, Y_all);\n",
    "length(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e., there are 12000 items in the data, which are either Coats or Sandals.\n",
    "\n",
    "For training let us consider a subsample of size $n=50$. \n",
    "\n",
    "Since the goal of this notebook is to illustrate how classification works, we will not stick to the algorithm for approaching machine learning problems. We will simply take the first $n$ items in the data set (instead of choosing them randomly). We also take another $n$ items for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data into the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = l[1:n];\n",
    "test_indices = l[n+1:2*n];\n",
    "\n",
    "X_test = X_all[:, :, test_indices]\n",
    "Y_test = Y_all[test_indices];\n",
    "|\n",
    "X = X_all[:, :, train_indices]\n",
    "Y = Y_all[train_indices];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a subsample of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = rand(1:n, 8)\n",
    "println([\"$(labels[Y[i]])\" for i in k])\n",
    "[convert2image(FashionMNIST, X[:,:,i]) for i in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "\n",
    "Each input data point is an image consisting of grey values for $28\\times 28$ pixels.\n",
    "\n",
    "Thus the data lives in $\\mathbb R^D$ with $D=28^2 = 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 28^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the structure of the first input data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(X[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the input data consists of matrices. \n",
    "\n",
    "For using machine learning algorithm we first transform the input data points into vectors using the `vec` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = [Float64.(vec(X[:,:,i])) for i in 1:n];\n",
    "X_test_vec = [Float64.(vec(X_test[:,:,i])) for i in 1:n];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also normalize the data to account for numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = maximum(norm.(X_vec))\n",
    "X_vec = X_vec ./ M\n",
    "X_test_vec = X_test_vec ./ M;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this data by randomly projecting the points in $\\mathbb R^D$ to $\\mathbb R^2$. \n",
    "\n",
    "For instance, we can first center the data and then sample a random projection matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x̄ = mean(X_vec)\n",
    "Pr = randn(2, D)\n",
    "X_Pr = hcat(map(x -> Pr*(x - x̄), X_vec)...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply a scatter plot the the projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_Pr[1, Y .== l₁], X_Pr[2, Y .== l₁], \n",
    "            color = :indianred,\n",
    "            label = labels[l₁],\n",
    "            size = (800,300))\n",
    "scatter!(X_Pr[1, Y .== l₂], X_Pr[2, Y .== l₂], \n",
    "            color = :steelblue,\n",
    "            label = labels[l₂],\n",
    "            size = (600,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "The first attempt to learn an algorithm for classification is using an SVM.\n",
    "\n",
    "We will use the simple kernel that gives back the standard inner product between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "κ(x₁,x₂) = x₁ ⋅ x₂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel defines a kernel matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = [κ(x₁,x₂) for (x₁,x₂) in \n",
    "        Iterators.product(X_vec,X_vec)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to transform the output data so that it takes values in $\\{-1,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = map(Y) do Yᵢ\n",
    "    if Yᵢ==l₁\n",
    "        -1\n",
    "    elseif Yᵢ==l₂\n",
    "        1\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to set up the optimization problem solving the Dual SVM.\n",
    "\n",
    "The regularization parameter is set to $C=5$.\n",
    "\n",
    "We use [JuMP](https://jump.dev/JuMP.jl/stable/) for setting up the model and [NLopt](https://github.com/JuliaOpt/NLopt.jl) for solving the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5;\n",
    "model = Model(NLopt.Optimizer)\n",
    "\n",
    "set_optimizer_attribute(model, \"algorithm\", :AUGLAG)\n",
    "set_optimizer_attribute(model, \"local_optimizer\", :LD_LBFGS)\n",
    "\n",
    "@variable(model, α[1:n]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraints are defined next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@constraint(model, c₁, sum(y .* α) .== 0);\n",
    "@constraint(model, c₂, α .>= zeros(n));\n",
    "@constraint(model, c₃, α .<= C .* ones(n));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@objective(model, Max, \n",
    "           4 * sum(α) - sum(y[i]*α[i]*y[j]*α[j]*G[i,j] \n",
    "            for (i,j) in Iterators.product(1:n,1:n)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following command we can now solve the optimization problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JuMP.optimize!(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the optimal value to define $\\psi$ and $b$ for our SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α_opt = value.(α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α_opt = value.(α)\n",
    "ψ(x) =  sum(0.5 * y[i] * α_opt[i] * κ(x, X_vec[i]) for i in 1:n)\n",
    "b = median(y[i] - ψ(X_vec[i]) for i in 1:n if abs(α_opt[i]) > 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the following function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_class(x) = Int(sign(ψ(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `f_class` in a prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(f, x)\n",
    "    if f(x) == -1\n",
    "        println(\"This is a $(labels[l₁]):\")\n",
    "    elseif f(x) == 1\n",
    "         println(\"This is a $(labels[l₂]):\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well does our prediction function work?\n",
    "\n",
    "We can now check the quality of our prediction function.\n",
    "\n",
    "For instance, we can randomly choose items from the test data and let our algorithm predict what item it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ℓ = rand(1:n)\n",
    "predict(f_class, X_test_vec[ℓ])\n",
    "convert2image(FashionMNIST, X_test[:,:,ℓ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second check, we plot projected test data points along with our predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x̄_test = mean(X_test_vec);\n",
    "X_test_Pr = hcat(map(x -> Pr*(x - x̄_test), X_test_vec)...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this, we produce $y_{\\mathrm{test}}\\in\\{-1,1\\}$ for the true labels and $\\hat{y}_{\\mathrm{test}}$ is the vector of predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = map(Y_test) do y\n",
    "    if y==l₁\n",
    "        -1\n",
    "    elseif y==l₂\n",
    "        1\n",
    "    end\n",
    "    end;\n",
    "ŷ_test = map(f_class, X_test_vec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization comes next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_test_Pr[1, y_test .== -1], X_test_Pr[2, y_test .== -1], \n",
    "            color = :indianred,\n",
    "            label = labels[l₁],\n",
    "            size = (600,300),\n",
    "            subplot = 1, layout = 2,\n",
    "            xlabel = \"true labels test data\")\n",
    "scatter!(X_test_Pr[1, y_test .== 1], X_test_Pr[2, y_test .== 1], \n",
    "            color = :steelblue,\n",
    "            label = labels[l₂],\n",
    "            subplot = 1, layout = 2)\n",
    "scatter!(X_test_Pr[1, ŷ_test .== -1], X_test_Pr[2, ŷ_test .== -1], \n",
    "            color = :indianred,\n",
    "            label = labels[l₁],\n",
    "            size = (600,300),\n",
    "            subplot = 2, layout = 2,\n",
    "            xlabel = \"predictions test data\")\n",
    "scatter!(X_test_Pr[1, ŷ_test .== 1], X_test_Pr[2, ŷ_test .== 1], \n",
    "            color = :steelblue,\n",
    "            label = labels[l₂],\n",
    "            subplot = 2, layout = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraction of uncorrectly prediced data points is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count((y_test-ŷ_test) .!= 0)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "We will now compute a classification algorithm using a neural network.\n",
    "\n",
    "Instead of setting up the network by hand as we did in Notebook 4, we will use [Flux.jl](https://fluxml.ai).\n",
    "\n",
    "We set up a neural network with three layers.\n",
    "\n",
    "The first layer is $\\mathbb R^D\\to\\mathbb R^{N_1}$. \n",
    "\n",
    "The second layer is $\\mathbb R^{N_1}\\to\\mathbb R^{N_2}$. \n",
    "\n",
    "The third layer is $\\mathbb R^{N_2}\\to\\mathbb R^{2}$.\n",
    "\n",
    "The output in $\\mathbb R^{2}$ will be a discrete probability distribution for our two items. Therefore, we use as the activation function in the last layer the softmax function. The other activation functions are chosen to be the ReLu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N₁, N₂ = 20, 15\n",
    "\n",
    "NN = Flux.Chain(\n",
    "          Dense(28^2, N₁, relu),\n",
    "          Dense(N₁, N₂, relu),\n",
    "          Dense(N₂, 2),\n",
    "          softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to modify the output data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y₀ = map(Y) do Yᵢ\n",
    "    if Yᵢ==l₁\n",
    "        [1;0]\n",
    "    elseif Yᵢ==l₂\n",
    "        [0;1]\n",
    "    end\n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As loss function we take the [crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) function `ℓ(p,q) = -p₁ log(q₁) - p₂ log(q₂).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.crossentropy(NN(x), y; dims = 2);\n",
    "# in Flux.jl the first variable is in ℝ^D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework to set up a Flux.jl' model is as follows:\n",
    "\n",
    "(The code is adapted from the lecture on [Neural Nets](https://github.com/JuliaAcademy/DataScience/blob/main/10.%20Neural%20Nets.ipynb) by [Huda Nassar](https://github.com/nassarhuda).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = Flux.params(NN);\n",
    "dataset = [(Float32.(x),y) for (x,y) in zip(X_vec, y₀)];\n",
    "opt = ADAM();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the trained using Flux.jl's `train!` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flux.@epochs 3 Flux.train!(loss, ps, dataset, opt)\n",
    "for i in 1:3\n",
    "    Flux.train!(loss, ps, dataset, opt) #train for 3 data loops, one loop is called an epoch, @epoch is outdated\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see what the algorithm that we have learned predicts on randomly chosen elements from the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ℓ = rand(1:n)\n",
    "p = NN(X_test_vec[ℓ])\n",
    "println(\"With probability $(p[1]): Coat.\\n\")\n",
    "println(\"With probability $(p[2]): Sandal.\\n\")\n",
    "convert2image(FashionMNIST, X_test[:,:,ℓ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider again the projected data and compare the labels from the neural network to the true labels on the test data.\n",
    "\n",
    "The size of the points should be proportional to the probability returned from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = map(x->maximum(NN(x)), X_test_vec) .* 6\n",
    "ŷ₀_test = map(x->argmax(NN(x)), X_test_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_test_Pr[1, y_test .== -1], X_test_Pr[2, y_test .== -1], \n",
    "            color = :indianred,\n",
    "            label = labels[l₁],\n",
    "            size = (600,300),\n",
    "            subplot = 1, layout = 2,\n",
    "            xlabel = \"true labels test data\")\n",
    "scatter!(X_test_Pr[1, y_test .== 1], X_test_Pr[2, y_test .== 1], \n",
    "            color = :steelblue,\n",
    "            label = labels[l₂],\n",
    "            subplot = 1, layout = 2)\n",
    "scatter!(X_test_Pr[1, ŷ₀_test .== 1], X_test_Pr[2, ŷ₀_test .== 1], \n",
    "            color = :indianred,\n",
    "            label = labels[l₁], subplot = 2,\n",
    "            size = (600,300), markersizes = sizes[ŷ₀_test .== 1],\n",
    "            xlabel = \"predictions test data\")\n",
    "scatter!(X_test_Pr[1, ŷ₀_test .== 2], X_test_Pr[2, ŷ₀_test .== 2], \n",
    "            color = :steelblue, subplot = 2, markersizes = sizes[ŷ₀_test .== 2],\n",
    "            label = labels[l₂])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count((y_test- 2 .* (ŷ₀_test .- 1.5)) .!= 0)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
